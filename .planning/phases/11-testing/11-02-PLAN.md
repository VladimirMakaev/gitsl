---
phase: 11-testing
plan: 02
type: execute
wave: 2
depends_on: ["01"]
files_modified:
  - tests/test_edge_cases.py
  - tests/test_error_conditions.py
  - tests/mocks/sl
  - tests/mocks/sl.cmd
autonomous: true

must_haves:
  truths:
    - "Tests verify special character filenames (spaces, unicode) work correctly"
    - "Tests verify empty repository behavior"
    - "Tests verify large file scenarios (many files, large content)"
    - "Tests verify error handling when sl binary is missing"
    - "Tests verify error handling for invalid arguments"
    - "Mock sl binary can simulate failures"
  artifacts:
    - path: "tests/test_edge_cases.py"
      provides: "Edge case tests for special characters, empty repos, and large file scenarios"
      min_lines: 80
    - path: "tests/test_error_conditions.py"
      provides: "Error condition tests for missing sl and invalid args"
      min_lines: 40
    - path: "tests/mocks/sl"
      provides: "Mock sl script for controlled testing"
      min_lines: 10
  key_links:
    - from: "tests/test_error_conditions.py"
      to: "tests/mocks/sl"
      via: "PATH injection for mock sl"
      pattern: "PATH"
    - from: "tests/test_edge_cases.py"
      to: "conftest.py fixtures"
      via: "uses sl_repo and sl_repo_with_commit"
      pattern: "sl_repo"
---

<objective>
Add comprehensive edge case and error condition tests

Purpose: Expand test coverage to include special character handling, empty repository behavior, large file scenarios, missing sl binary scenarios, and invalid argument handling. Addresses coverage gaps identified in research.

Output:
- `tests/test_edge_cases.py` - Tests for special characters, empty repos, large files
- `tests/test_error_conditions.py` - Tests for missing sl, invalid args
- `tests/mocks/sl` - Mock sl script for error simulation
- `tests/mocks/sl.cmd` - Windows wrapper for mock sl
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-testing/11-CONTEXT.md
@.planning/phases/11-testing/11-RESEARCH.md
@.planning/phases/11-testing/11-01-SUMMARY.md
@tests/conftest.py
@tests/helpers/commands.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create edge case tests</name>
  <files>tests/test_edge_cases.py</files>
  <action>
Create `tests/test_edge_cases.py` with comprehensive edge case tests:

1. File structure:
   - Import shutil, pytest, conftest helpers
   - Set `sl_available` check and skip marker
   - Add `pytest.mark.always` so these run with any command filter

2. Special character tests (`TestSpecialCharacters` class):
   - `test_filename_with_spaces`: Create and add file with spaces in name
   - `test_filename_with_unicode`: Create and add file with accented characters (e.g., `file_\u00e9.txt`)
   - `test_filename_with_brackets`: Create and add file with brackets `file[1].txt`
   - `test_status_with_special_filenames`: Verify status output handles special chars

3. Empty repository tests (`TestEmptyRepo` class):
   - `test_status_empty_repo`: Run `status --porcelain` on repo with no files
   - `test_status_short_empty_repo`: Run `status -s` on empty repo
   - `test_log_empty_repo`: Run `log` on repo with no commits (verify graceful handling)
   - `test_diff_empty_repo`: Run `diff` on repo with no commits

4. Large file scenario tests (`TestLargeFileScenarios` class):
   - `test_many_files`: Create 100+ files in repo, run `status --porcelain`, verify all listed correctly
   - `test_large_file_content`: Create file with 10MB+ content, add and verify status shows it

5. Path edge cases (`TestPathEdgeCases` class):
   - `test_add_from_subdirectory`: Add file using relative path from subdirectory
   - `test_status_in_subdirectory`: Run status from subdirectory

Use existing fixtures: `sl_repo` (empty), `sl_repo_with_commit` (has content)
  </action>
  <verify>
Run `pytest tests/test_edge_cases.py -v` and verify all tests pass or skip appropriately.
  </verify>
  <done>
- TEST-05 SATISFIED: Tests for edge cases exist and pass
- Special character tests cover spaces, unicode, brackets
- Empty repo tests cover status, log, diff
- Large file tests cover many files (100+) and large content (10MB+)
- Path tests cover subdirectory scenarios
  </done>
</task>

<task type="auto">
  <name>Task 2: Create mock sl binary and error condition tests</name>
  <files>tests/mocks/sl, tests/mocks/sl.cmd, tests/test_error_conditions.py</files>
  <action>
Create mock sl infrastructure:

1. Create `tests/mocks/` directory
2. Create `tests/mocks/sl` Python script:
```python
#!/usr/bin/env python3
"""Mock sl binary for testing error conditions."""
import os
import sys

exit_code = int(os.environ.get("MOCK_SL_EXIT", "0"))
stdout = os.environ.get("MOCK_SL_STDOUT", "")
stderr = os.environ.get("MOCK_SL_STDERR", "")

if stdout:
    print(stdout)
if stderr:
    print(stderr, file=sys.stderr)
sys.exit(exit_code)
```
Make executable: `chmod +x tests/mocks/sl`

3. Create `tests/mocks/sl.cmd` for Windows:
```batch
@echo off
python "%~dp0sl" %*
```

4. Create `tests/test_error_conditions.py`:
   - Import os, pytest, Path, helpers
   - Add `pytest.mark.always` marker

5. Mock sl fixture:
```python
@pytest.fixture
def mock_sl_env(tmp_path):
    """Environment with mock sl prepended to PATH."""
    mock_dir = Path(__file__).parent / "mocks"
    new_path = str(mock_dir) + os.pathsep + os.environ.get("PATH", "")
    return {"PATH": new_path}
```

6. Error condition tests (`TestSlErrors` class):
   - `test_sl_returns_nonzero`: Mock sl exits with error, verify gitsl propagates
   - `test_sl_stderr_propagates`: Mock sl outputs to stderr, verify gitsl passes through

7. Invalid argument tests (`TestInvalidArgs` class):
   - `test_unknown_command_handled`: Run gitsl with unknown command, verify unsupported message
   - `test_no_command_shows_usage_or_error`: Run gitsl with no args, verify behavior

8. Missing sl tests (if feasible):
   - Create fixture with empty PATH (or PATH without sl)
   - Test gitsl behavior when sl truly not found
   - Note: May need to skip on CI where sl must be present
  </action>
  <verify>
Run `pytest tests/test_error_conditions.py -v` and verify tests pass.
Verify mock sl is executable: `./tests/mocks/sl`
  </verify>
  <done>
- TEST-06 SATISFIED: Tests for error conditions exist and pass
- Mock sl binary created and works cross-platform
- Error propagation from sl tested
- Invalid argument handling tested
  </done>
</task>

<task type="auto">
  <name>Task 3: Audit and add missing flag combination tests</name>
  <files>tests/test_status.py, tests/test_log.py, tests/test_add.py</files>
  <action>
Based on RESEARCH.md coverage gap analysis, add missing flag combination tests to existing test files.

NOTE: Plan 01 renames test files. Use the new names:
- `test_status.py` (renamed from `test_status_porcelain.py`)
- `test_log.py` (renamed from `test_cmd_log.py`)
- `test_add.py` (renamed from `test_cmd_add.py`)
- `test_diff.py` (renamed from `test_cmd_diff.py`)

1. In status tests (`tests/test_status.py`):
   - Add test for `status -s` combined with other flags if any
   - Verify all porcelain format codes tested

2. In log tests (`tests/test_log.py`):
   - Add test for `log -5 --oneline` (combined flags)
   - Add test for `log` with no flags on repo with commits

3. In add tests (`tests/test_add.py`):
   - Verify `-A` and `--all` both tested (already done)
   - Add test for `add .` (current directory)
   - Add test for `add` with glob pattern if supported

4. In diff tests (`tests/test_diff.py`):
   - Add test for `diff` with specific file path
   - Add test for `diff` with staged changes

These are incremental additions to existing test classes, not new files.
  </action>
  <verify>
Run `pytest tests/ -v` and verify increased test count.
Run `pytest tests/ --collect-only | wc -l` before and after to compare.
  </verify>
  <done>
- TEST-04 SATISFIED: Existing tests audited for completeness
- TEST-07 SATISFIED: Flag combinations tested
- Combined flag usage verified (e.g., `-5 --oneline`)
- Additional file/path scenarios covered
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/test_edge_cases.py` passes (or skips gracefully without sl)
2. `pytest tests/test_error_conditions.py` passes
3. Mock sl script works: `MOCK_SL_EXIT=1 ./tests/mocks/sl` returns exit code 1
4. All existing tests still pass
5. Test count has increased from baseline
6. Large file tests complete within reasonable time (< 30 seconds)
</verification>

<success_criteria>
- TEST-04 SATISFIED: Existing tests audited, gaps filled
- TEST-05 SATISFIED: Edge cases (empty repos, special characters, large files) covered
- TEST-06 SATISFIED: Error conditions (missing sl, invalid args) covered
- TEST-07 SATISFIED: Flag combinations not previously covered now tested
</success_criteria>

<output>
After completion, create `.planning/phases/11-testing/11-02-SUMMARY.md`
</output>
